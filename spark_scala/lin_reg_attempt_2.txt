// Start Spark
bin\spark-shell --packages com.databricks:spark-csv_2.11:1.3.0


// Load Required Libraries
import org.apache.spark.ml.regression.LinearRegression
import org.apache.spark.ml.feature.VectorAssembler
import org.apache.spark.mllib.linalg.Vectors



// Import Data (CSV)
val quakes = sqlContext.read.format("com.databricks.spark.csv").option("header", "true").option("inferSchema", "true").load("C:/Users/yassin.eltahir/quakes.csv")



// Create Features & Labels

val assembler = new VectorAssembler()
  .setInputCols(Array("lat", "long", "depth","stations"))
  .setOutputCol("features")

val output = assembler.transform(quakes)

val mod_dat = output.select("features","mag")









##### Everything Below This Point Is Crap!!  #####




val test = mod_dat.toDF("features","label")



// Build Model
val lr = new LinearRegression()
  .setMaxIter(10)
  .setRegParam(0.3)
  .setElasticNetParam(0.8)


// Fit Model
val lrModel = lr.fit(mod_dat)
val lrModel = lr.fit(training)




// sc is an existing SparkContext.
val sqlContext = new org.apache.spark.sql.SQLContext(sc)



val training = sqlContext.read.format("libsvm").load("data/mllib/sample_linear_regression_data.txt")








val dataset = sqlContext.createDataFrame(
  Seq((0, 18, 1.0, Vectors.dense(0.0, 10.0, 0.5), 1.0))
).toDF("id", "hour", "mobile", "userFeatures", "clicked")

println(output.select("features", "clicked").first())